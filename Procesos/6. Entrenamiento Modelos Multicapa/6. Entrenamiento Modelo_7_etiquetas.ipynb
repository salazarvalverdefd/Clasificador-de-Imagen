{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerías de keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "#preprocesamiento de imagen\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \"\"\"\n",
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=7)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8030 images belonging to 7 classes.\n",
      "Found 2415 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "c:\\users\\dick\\2\\venv\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=250, epochs=25, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 158s 633ms/step - loss: 0.3341 - accuracy: 0.8653\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 92s 367ms/step - loss: 0.2857 - accuracy: 0.8842\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 93s 373ms/step - loss: 0.2615 - accuracy: 0.8956\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 97s 387ms/step - loss: 0.2410 - accuracy: 0.9028\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2271 - accuracy: 0.9087\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 92s 370ms/step - loss: 0.2145 - accuracy: 0.9136\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 96s 386ms/step - loss: 0.2043 - accuracy: 0.9181\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.1957 - accuracy: 0.9229\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 92s 368ms/step - loss: 0.1866 - accuracy: 0.9264\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 97s 389ms/step - loss: 0.1779 - accuracy: 0.9293\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.1689 - accuracy: 0.9342\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 91s 366ms/step - loss: 0.1653 - accuracy: 0.9349\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 102s 406ms/step - loss: 0.1547 - accuracy: 0.9391\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 103s 413ms/step - loss: 0.1502 - accuracy: 0.9410\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.1417 - accuracy: 0.9445\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 95s 379ms/step - loss: 0.1402 - accuracy: 0.9452\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.1356 - accuracy: 0.9471\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 93s 372ms/step - loss: 0.1256 - accuracy: 0.9511\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 95s 380ms/step - loss: 0.1224 - accuracy: 0.9520\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 94s 374ms/step - loss: 0.1195 - accuracy: 0.9535\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 95s 382ms/step - loss: 0.1100 - accuracy: 0.9585\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 95s 382ms/step - loss: 0.1041 - accuracy: 0.9607\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 94s 376ms/step - loss: 0.1037 - accuracy: 0.9594\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 95s 379ms/step - loss: 0.0994 - accuracy: 0.9615\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 90s 358ms/step - loss: 0.0952 - accuracy: 0.9627\n",
      "Wall time: 40min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18c04c3f908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### ENTRENAR MODELO PRIMER NIVEL ####\n",
    "numClases=7\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = numClases, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('2. Dataset_Imágenes_No Vacías-7/trainingSet',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('2. Dataset_Imágenes_No Vacías-7/testSet',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 25,\n",
    "                         nb_val_samples = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 545 images belonging to 1 classes.\n",
      "[[1.89244747e-04 1.10268593e-06 1.48590505e-02 7.34755099e-02\n",
      "  1.08659267e-04 1.78813934e-05 9.95014369e-01]\n",
      " [2.16387212e-02 1.25379860e-02 4.06035781e-03 1.94817305e-01\n",
      "  5.81353903e-03 1.01424098e-01 3.74862075e-01]\n",
      " [2.94029713e-04 1.12536550e-03 1.52885914e-05 6.23004138e-02\n",
      "  6.42240047e-05 2.59569287e-03 9.10331309e-02]\n",
      " [4.05311584e-06 7.90992379e-03 1.01599097e-03 1.20856464e-02\n",
      "  2.67416239e-04 2.04145908e-05 9.54119682e-01]\n",
      " [9.00030136e-06 2.75492668e-04 1.46746635e-04 2.68372893e-03\n",
      "  2.05069780e-04 6.46710396e-06 9.97664928e-01]\n",
      " [3.54647636e-06 4.17304337e-02 4.52995300e-06 2.03856826e-03\n",
      "  9.90523338e-01 0.00000000e+00 4.20212746e-05]\n",
      " [5.96046448e-08 2.23517418e-06 7.15255737e-07 6.12777591e-01\n",
      "  8.34465027e-07 2.20537186e-04 9.95162606e-01]\n",
      " [2.62796879e-04 1.10089779e-04 6.76810741e-05 1.64419413e-04\n",
      "  1.21012330e-03 0.00000000e+00 9.60161686e-01]\n",
      " [2.21489608e-01 7.74860382e-07 3.87430191e-07 3.92001867e-03\n",
      "  2.50965357e-04 1.06540322e-03 7.60445595e-02]\n",
      " [2.00361013e-04 5.87380509e-06 6.17004189e-05 4.25748497e-01\n",
      "  2.16840941e-04 6.04039269e-06 1.84396282e-01]]\n",
      "loss [1.89244747e-04 1.10268593e-06 1.48590505e-02 7.34755099e-02\n",
      " 1.08659267e-04 1.78813934e-05 9.95014369e-01]\n",
      "accuracy [0.02163872 0.01253799 0.00406036 0.1948173  0.00581354 0.1014241\n",
      " 0.37486207]\n"
     ]
    }
   ],
   "source": [
    "#### TEST SHAMPOO ####\n",
    "test_datagen2 = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen2.flow_from_directory('2. Dataset_Imágenes_No Vacías-7/testSet',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 10,\n",
    "                                            classes=['Shampoo'],\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "\n",
    "\n",
    "resultDog=classifier.predict_generator(test_set,steps=1)\n",
    "print(resultDog)\n",
    "for name, value in zip(classifier.metrics_names, resultDog):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "inputs:  ['conv2d_3_input']\n",
      "outputs:  ['dense_4/Sigmoid']\n",
      "WARNING:tensorflow:From <ipython-input-6-9075827225a2>:17: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From c:\\users\\dick\\2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 78 variables.\n",
      "INFO:tensorflow:Converted 78 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Model_TF1_13_1/7-classifier_1_13_1.pb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#### GUARDAR MODELO FORMATO TENSORFLOW ####\n",
    "\n",
    "print(tf.__version__)\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = ''\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "print('inputs: ', [input.op.name for input in classifier.inputs])\n",
    "\n",
    "print('outputs: ', [output.op.name for output in classifier.outputs])\n",
    "\n",
    "classifier.save('./Model_TF1_13_1/7-classifier_1_13_1.h5')\n",
    "\n",
    "frozen_graph = freeze_session(tf.keras.backend.get_session(), output_names=[out.op.name for out in classifier.outputs])\n",
    "tf.train.write_graph(frozen_graph, './Model_TF1_13_1/', '7-classifier_1_13_1.pbtxt', as_text=True)\n",
    "tf.train.write_graph(frozen_graph, './Model_TF1_13_1/', '7-classifier_1_13_1.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 775 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./Model_TF1_13_1/classifier_1_13_1.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#### GUARDAR MODELO FORMATO JOBLIB ####\n",
    "from joblib import dump, load\n",
    "dump(classifier, './Model_TF1_13_1/7-classifier_1_13_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PRUEBA CORRECTO GUARDADO FORMATO JOBLIB ####\n",
    "classifier = load('./Model_TF1_13_1/7-classifier_1_13_1.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
